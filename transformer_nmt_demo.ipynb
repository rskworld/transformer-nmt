{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer-based Neural Machine Translation\n",
        "\n",
        "**Author:** Molla Samser  \n",
        "**Website:** https://rskworld.in  \n",
        "**Email:** help@rskworld.in, support@rskworld.in  \n",
        "**Phone:** +91 93305 39277  \n",
        "**Designer & Tester:** Rima Khatun\n",
        "\n",
        "This notebook demonstrates a complete Transformer-based neural machine translation system implementing:\n",
        "- Multi-head self-attention mechanism\n",
        "- Positional encoding\n",
        "- Encoder-decoder architecture\n",
        "- Beam search for high-quality translation generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\"\"\"\n",
        "Author: Molla Samser (https://rskworld.in)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformer_model import Transformer\n",
        "from data_preprocessing import Vocabulary, normalize_string, load_data\n",
        "from inference import translate_sentence, greedy_decode, load_model\n",
        "import os\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Model Architecture Visualization\n",
        "\n",
        "Let's create and examine the transformer model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a sample transformer model\n",
        "\"\"\"\n",
        "Author: Molla Samser (https://rskworld.in)\n",
        "\"\"\"\n",
        "\n",
        "# Model hyperparameters\n",
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_len = 100\n",
        "dropout = 0.1\n",
        "\n",
        "# Create model\n",
        "model = Transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    num_encoder_layers=num_layers,\n",
        "    num_decoder_layers=num_layers,\n",
        "    d_ff=d_ff,\n",
        "    max_len=max_len,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding Self-Attention Mechanism\n",
        "\n",
        "Let's visualize how self-attention works by creating a simple example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate self-attention mechanism\n",
        "\"\"\"\n",
        "Author: Molla Samser (https://rskworld.in)\n",
        "\n",
        "This cell demonstrates the self-attention mechanism by showing\n",
        "how words in a sentence attend to each other.\n",
        "\"\"\"\n",
        "\n",
        "from transformer_model import MultiHeadAttention\n",
        "\n",
        "# Example: Simple attention visualization\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "batch_size = 1\n",
        "seq_len = 5\n",
        "\n",
        "# Create sample input (batch_size, seq_len, d_model)\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# Create attention module\n",
        "attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# Compute attention\n",
        "output, attention_weights = attn(x, x, x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"\\nSelf-attention allows each word to attend to all words in the sequence,\")\n",
        "print(f\"capturing relationships regardless of distance.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training the Model (Example)\n",
        "\n",
        "Note: This is a demonstration. For actual training, use the `train.py` script with your parallel corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example training setup (commented out for demo)\n",
        "\"\"\"\n",
        "Author: Molla Samser (https://rskworld.in)\n",
        "\n",
        "To train the model, you would:\n",
        "1. Prepare parallel corpus in format: source_sentence ||| target_sentence\n",
        "2. Run: python train.py --data_path your_data.txt --num_epochs 50 --batch_size 32\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training example:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"python train.py \\\\\")\n",
        "print(\"    --data_path data/parallel_corpus.txt \\\\\")\n",
        "print(\"    --num_epochs 50 \\\\\")\n",
        "print(\"    --batch_size 32 \\\\\")\n",
        "print(\"    --d_model 512 \\\\\")\n",
        "print(\"    --num_heads 8 \\\\\")\n",
        "print(\"    --num_layers 6 \\\\\")\n",
        "print(\"    --lr 0.0001 \\\\\")\n",
        "print(\"    --save_dir ./models\")\n",
        "print(\"\\nThis will:\")\n",
        "print(\"1. Load and preprocess the parallel corpus\")\n",
        "print(\"2. Build source and target vocabularies\")\n",
        "print(\"3. Train the transformer model\")\n",
        "print(\"4. Save the best model checkpoint\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Translation Inference\n",
        "\n",
        "If you have a trained model, you can use it for translation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translation example\n",
        "\"\"\"\n",
        "Author: Molla Samser (https://rskworld.in)\n",
        "\n",
        "This demonstrates how to use a trained model for translation.\n",
        "Uncomment and modify if you have a trained model.\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Example usage (commented out - requires trained model):\n",
        "\"\"\"\n",
        "# Load vocabularies\n",
        "from data_preprocessing import load_vocabularies\n",
        "src_vocab, tgt_vocab = load_vocabularies('./models')\n",
        "\n",
        "# Load model\n",
        "model = load_model('./models/best_model.pt', device)\n",
        "\n",
        "# Translate a sentence\n",
        "source_sentence = \"Hello, how are you?\"\n",
        "translation = translate_sentence(\n",
        "    source_sentence,\n",
        "    './models/best_model.pt',\n",
        "    './models',\n",
        "    device=device,\n",
        "    use_beam_search=True,\n",
        "    beam_width=5\n",
        ")\n",
        "\n",
        "print(f\"Source: {source_sentence}\")\n",
        "print(f\"Translation: {translation}\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"Translation Inference:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"To translate sentences, use:\")\n",
        "print(\"1. Single sentence: python inference.py --model_path models/best_model.pt --sentence 'Your sentence here'\")\n",
        "print(\"2. File translation: python inference.py --model_path models/best_model.pt --input_file input.txt --output_file output.txt\")\n",
        "print(\"\\nOptions:\")\n",
        "print(\"- --use_beam_search: Use beam search (better quality, slower)\")\n",
        "print(\"- --beam_width: Number of beams (default: 5)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Components Explanation\n",
        "\n",
        "### 5.1 Positional Encoding\n",
        "\n",
        "The positional encoding adds information about the position of words in the sequence, since transformers don't have inherent notion of order.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize positional encoding\n",
        "\"\"\"\n",
        "Author: Molla Samser (https://rskworld.in)\n",
        "\"\"\"\n",
        "\n",
        "from transformer_model import PositionalEncoding\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create positional encoding\n",
        "d_model = 128\n",
        "max_len = 50\n",
        "pos_encoding = PositionalEncoding(d_model, max_len, dropout=0)\n",
        "\n",
        "# Generate encoding for visualization\n",
        "x = torch.zeros(1, max_len, d_model)\n",
        "pe = pos_encoding(x)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(pe[0].numpy().T, aspect='auto', cmap='RdYlGn')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Dimension')\n",
        "plt.title('Positional Encoding Pattern')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Positional encoding adds unique patterns for each position,\")\n",
        "print(\"allowing the model to understand word order.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Features\n",
        "\n",
        "1. **Self-Attention**: Each word can attend to all words in the sequence\n",
        "2. **Multi-Head Attention**: Multiple attention heads capture different types of relationships\n",
        "3. **Positional Encoding**: Adds positional information to embeddings\n",
        "4. **Encoder-Decoder**: Separate encoder and decoder for source and target languages\n",
        "5. **Residual Connections**: Helps with training deep networks\n",
        "6. **Layer Normalization**: Stabilizes training\n",
        "7. **Beam Search**: Generates high-quality translations\n",
        "\n",
        "## 7. Project Structure\n",
        "\n",
        "```\n",
        "transformer-nmt/\n",
        "├── transformer_model.py      # Core transformer architecture\n",
        "├── data_preprocessing.py     # Data loading and preprocessing\n",
        "├── train.py                  # Training script\n",
        "├── inference.py              # Inference and translation\n",
        "├── transformer_nmt_demo.ipynb # This notebook\n",
        "├── requirements.txt          # Python dependencies\n",
        "├── README.md                 # Project documentation\n",
        "└── models/                   # Saved models and vocabularies\n",
        "```\n",
        "\n",
        "## 8. Contact Information\n",
        "\n",
        "**Author:** Molla Samser  \n",
        "**Website:** https://rskworld.in  \n",
        "**Email:** help@rskworld.in, support@rskworld.in  \n",
        "**Phone:** +91 93305 39277  \n",
        "**Designer & Tester:** Rima Khatun\n",
        "\n",
        "For more projects and resources, visit: https://rskworld.in\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
